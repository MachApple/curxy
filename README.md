# curxy

#### *cursor* + *proxy* = **curxy**

An proxy worker for using ollama in cursor

## What is this?

This is a proxy worker for using ollama in cursor. It is a simple server that forwards requests to the ollama server and returns the response.

## Why do you need this?

When we use llm prediction on cusor editor, the editor sends to the data to the official cursor server, and the server sends the data to the ollama server. 
Therefore, even if the endpoint is set to localhost in the cursor editor configuration, the cursor server cannot send communication to the local server.
So, we need a proxy worker that can forward the data to the ollama server.

## requirements

- deno
- ollama server
- cloudflared

## How to use

```sh
deno run -A https://raw.githubusercontent.com/ryoppippi/proxy-worker-for-cursor-with-ollama/main/main.ts
```

if you limit the access to the ollama server, you can set `OPENAI_API_KEY` environment variable.

```sh
OPENAI_API_KEY=your_openai_api_key deno run -A https://raw.githubusercontent.com/ryoppippi/proxy-worker-for-cursor-with-ollama/main/main.ts
```

Then, you can see the URL generated by the cloudflared. You can set this URL to the endpoint of the cursor editor.

## License

MIT
